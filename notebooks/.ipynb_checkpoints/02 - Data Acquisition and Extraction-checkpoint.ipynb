{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key to effective scraping is to understand how content and data is stored on web servers, how to identify the data you want to retrieve, an understand how the tools support this extraction.  In this chapter we will discuss website structures and the DOM, introduce techniques to parse and query websites with lxml, XPath and CSS.  We will also look at how to work with websites developed in other languages and different encoding types such as Unicode.\n",
    "\n",
    "Ultimately, understanding how to find and extract data within an HTML document comes down to understanding the structure of the HTML page, it's representation with DOM, the process of querying the DOM for specific elements, and how to specify which elements you need want to retrieve based upon how the data is represented.\n",
    "\n",
    "In this chapter we will cover:\n",
    "* How to parse websites & navigate tree\n",
    "* How to parse XML & HTML with lxml\n",
    "* Dealing with children, parents, sibling and attributes\n",
    "* Query data with XPath\n",
    "* Query data with CSS\n",
    "* Beautiful Soap’s find methods\n",
    "* Selectors in Scrapy\n",
    "* Handling HTML in UTF-8 format\n",
    "\n",
    "## Skills Learned\n",
    "* Develop an understanding of the structure of a web page when represented with the DOM\n",
    "* Learn how to navigate through DOM elements using children, siblings and parents\n",
    "* Learn the fundamentals of XPath, and understand how it can be used to find specific pieces of data within an HTML document.\n",
    "* Be able to query and extract data with XPath, CSS and Regular expression\n",
    "* Know how to handle Unicode and UTF-8 encodings for docouments\n",
    "\n",
    "## References\n",
    "[Planetary Facts](https://nssdc.gsfc.nasa.gov/planetary/factsheet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to parse websites & navigate tree, dealing with children, parents, sibling and attributes\n",
    "\n",
    "When the browser displays a web page it builds a model of the content of the page in a representation known as the DOM (document object model).  The DOM is a hierarchical representation of all content of the page, as well as structural information, style information, scripts and links to other content.  It is critical to understand this structure to be able to effectively scrape data from web pages.  We will look at an example web page, its DOM, and examine how to navigate the DOM with beautiful soup.\n",
    "\n",
    "## Getting ready\n",
    "It is possible to examine the DOM in Chrome by right clicking the page and selecting Inspect.  The following shows opening inspection on the page http://127.0.0.1:8080/pages/planets.min.html \n",
    "\n",
    "![](img/01_01.png)\n",
    "\n",
    "This opens the developer tools and the inspector.  The DOM can be examined in the elements tab.  The following shows the selection of the first row in the table.\n",
    "\n",
    "![](img/01_02.png)\n",
    "\n",
    "The tr element represents the row, and there are several characteristics of this element and its neighboring elements that we will examine.  First is that this element has three attributes: id, planet and name.  Attributes are often important in scraping as they commonly are used identify data embedded in the HTML.\n",
    "\n",
    "Second, the tr element has children, in this case the five td elements.  We will often need to look into the children of a specific element to find the actual data we desire.\n",
    "\n",
    "This element has a parent, <tbody>. It also has siblings, the other elements that are also children of the parent element.  As we will see, we can use constructs in XPath to navigate these relationships.\n",
    "\n",
    "## How to do it...\n",
    "\n",
    "We can load this page into a BeautifulSoup object using the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen(\"http://127.0.0.1:8080/pages/planets.min.html\")\n",
    "bsobj = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the resulting object bsobj reports the underlying HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bsobj.text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can navigate the elements in the DOM using properties of bsobj.  bsobj represents the overall document and we can drill into the document by chaining the tag names.  The following navigates to the table containing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bsobj.html.body.div.table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node has both children and descendants. Descendants are all the nodes underneath a given node, while children are those that are a first level descendant.  The following retrieves the children of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bsobj.html.body.div.table.children"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look over each child element using a for loop. The following will get all the children of the table element, each of which is a <tr>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for c in bsobj.html.body.div.table.children:\n",
    "    print (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful soup will always return the first descendant of an element when using tags as properties.  While the table has many rows, .tr only returns the first tr child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bsobj.html.body.div.table.tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From any given sibling, we can progress to the next sibling using .find_next_sibling()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bsobj.html.body.div.table.tr.find_next_sibling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following demonstrates iterating all descendants of the first tr element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for d in bsobj.html.body.div.table.tr.descendants:\n",
    "    print (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parent of a node can be found using the .parent property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bsobj.html.body.div.table.tr.parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works\n",
    "Beautiful soup converts the HTML from the page into it’s own internal representation.  This model has an identical representation to the DOM that would be created by a browser.  But beautiful soup also provides many powerful capabilities for navigating the elements in the DOM, such as what we have seen using the tags as properties \n",
    "\n",
    "## There's more...\n",
    "This manner of navigating the DOM is relatively inflexible and is highly dependent upon the structure.  It is possible that this structure can change over time as web pages are updated by their creator(s).  They could even look identical, but have a completely different structure that breaks your scraping code.\n",
    "\n",
    "So how can we deal with this?  As we will see, there are several ways of searching for elements that are much better than defining explicit paths.  In general, we can do this using XPath, and also using the find methods of beautiful soup.  We will examine both in further recipes in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# BeautifulSoup's find methods\n",
    "Items can also be located within the DOM using beautiful soups find methods.  These methods give us a much more flexible and powerful construct for finding elements that is not dependent upon the hierarchy of those elements, and which also provides us with search capabilities.\n",
    "\n",
    "We will examine several common uses of these functions to locate various elements in the DOM.\n",
    "\n",
    "## Getting ready\n",
    "We will start by loading the sample page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen(\"http://127.0.0.1:8080/pages/planets.min.html\")\n",
    "bsobj = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "In the previous example to access all of the tr elements that contained data we needed to get the table property and then either select the children of that element, or get the first tr descendant and iterate through all remaining siblings.  \n",
    "\n",
    "We can do this more effectively with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_tr = bsobj.html.body.div.table.findAll(\"tr\")\n",
    "all_tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, this has returned us all the tr elements that are descendants of the table with one simple statement, and also gave us them as a python list instead of an iterator.\n",
    "\n",
    "We can also specify that the attributes of the tag that we are searching for be a specific name and values.  The following retrieves tr elements with the an id=\"planet1\" attribute (Mercury)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mercury = bsobj.html.body.div.table.findAll(\"tr\", {\"id\": \"planet1\"})\n",
    "mercury"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!  And we used the fact that this page uses this attribute to represent table rows with actual data, with the result then also omitting the header tr element.  The following demonstrates by building a dictionary of the planets and their masses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "items_price = dict()\n",
    "planet_rows = bsobj.html.body.div.table.findAll(\"tr\", {\"class\": \"planet\"})\n",
    "for i in planet_rows:\n",
    "    tds = i.findAll(\"td\")\n",
    "    items_price[tds[1].text.strip()] = tds[2].text.strip()\n",
    "items_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works\n",
    "This works because the findAll performs a search for all DOM elements with the given name.  Under the covers, it likely converts the string provided to findAll to XPath.\n",
    "\n",
    "## There's more...\n",
    "Speaking of XPath, it's now time to cover it in some detail.  That's the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query with XPath\n",
    "The lxml XML toolkit is a Pythonic binding for the C libraries libxml2 and libxslt. It is unique in that it combines the speed and XML feature completeness of these libraries with the simplicity of a native Python API, mostly compatible but superior to the well-known ElementTree API. The latest release works with all CPython versions from 2.6 to 3.6. See the introduction for more information about background and goals of the lxml project. Some common questions are answered in the FAQ.\n",
    "\n",
    "Lxml is a Python wrapper on top of the libxml2 XML parsing library written in C, which helps make it faster than Beautiful Soup but also harder to install on some computers. The latest installation instructions are available at http://lxml.de/installation.html.\n",
    "\n",
    "lxml supports XPath which makes it considerably easy to manage complex XML and HTML documents.  We will examine several techniques of using lxml and XPath together, and how to use lxml and XPath to navigate the DOM and access data.\n",
    "\n",
    "## Getting ready\n",
    "Start by importing html from lxml, and also requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load the page with requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html>\\r\\n\\r\\n<head>\\r\\n</head>\\r\\n\\r\\n<body>\\r\\n    <div id=\"planets\">\\r\\n        <h1>Planetary data</h1>\\r\\n        <div id=\"content\">Here are some interesting facts about the planets in our solar system</div>\\r\\n        <p></p>\\r\\n        <table id=\"planetsTable\" border=\"1\">\\r\\n            <tr id=\"planetHeader\">\\r\\n                <th>\\r\\n                </th>\\r\\n                <th>\\r\\n                    Name\\r\\n                </th>\\r\\n                <th>\\r\\n                    Mass (10^24kg)\\r\\n                </th>\\r\\n                <th>\\r\\n                    Diameter (km)\\r\\n                </th>\\r\\n                <th>\\r\\n                    How it got its Name\\r\\n                </th>\\r\\n            </tr>\\r\\n\\r\\n            <tr id=\"planet1\" class=\"planet\" name=\"Mercury\">\\r\\n                <td>\\r\\n                    <img src=\"img/mercury-150x150.png\">\\r\\n                </td>\\r\\n                <td>\\r\\n                    Mercury\\r\\n                </td>\\r\\n                <td>\\r\\n                    0.330\\r\\n                </td>\\r\\n           '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page = requests.get(\"http://127.0.0.1:8080/pages/planets.html\")\n",
    "page.text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load the content into an element tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Element html at 0x10ec85cc8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = html.fromstring(page.content)\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tree variable is now an lxml representation of the DOM which models the HTML content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "The ultimate goal of this task is to learn about XPath and how to use it to extract data from HTML.  We will examine this specific document and look to extract data for the planets and learn various XPath concepts.\n",
    "\n",
    "Let's start with the following XPath."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element tr at 0x10ebe7ea8>\n",
      "<Element tr at 0x10ebe7c28>\n",
      "<Element tr at 0x10ebe7e58>\n",
      "<Element tr at 0x10ea48048>\n",
      "<Element tr at 0x10ea48bd8>\n",
      "<Element tr at 0x10ec767c8>\n",
      "<Element tr at 0x10ec76bd8>\n",
      "<Element tr at 0x10ec760e8>\n",
      "<Element tr at 0x10ec76958>\n",
      "<Element tr at 0x10ec76cc8>\n",
      "<Element tr at 0x10ec76c78>\n"
     ]
    }
   ],
   "source": [
    "for v in tree.xpath(\"/html/body/div/table/tr\"):\n",
    "    print (v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This XPath asks to return all the tr elements found from the root of the document and descending through tags with names html, body, div and table.\n",
    "\n",
    "This returned 11 tr elements.  This is perhaps a little curious as there are only 9 planets in the data, so why 11 rows?\n",
    "\n",
    "Let's examine by changing the statement slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['planetHeader']\n",
      "['planet1']\n",
      "['planet2']\n",
      "['planet3']\n",
      "['planet4']\n",
      "['planet5']\n",
      "['planet6']\n",
      "['planet7']\n",
      "['planet8']\n",
      "['planet9']\n",
      "['footerRow']\n"
     ]
    }
   ],
   "source": [
    "for v in tree.xpath(\"/html/body/div/table/tr\"):\n",
    "    print (v.xpath(\"@id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This modification prints the value of the id attribute of the tr rows that are found.  The XPath for an attribute of the current node is @ followed by the attribute name.\n",
    "\n",
    "So this is returning all the rows from two different tables.  At each level of the XPath (between any /'s) there can be multiple return values.  There are two div tags beneath body in this document.  So the XPath engine continues to look down the next level (to table) on all the found div elements.  The table then returns two tables, and then the process continues with finding all the tr elements on all the found tables.  So 9 planet rows, plus one header, and one footer row.\n",
    "\n",
    "So how can we select just the table and rows with planet data?  In this document, there are several ways (by design).\n",
    "\n",
    "Take the following as a first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element tr at 0x10eca3e08> ['planetHeader']\n",
      "<Element tr at 0x10eca3908> ['planet1']\n",
      "<Element tr at 0x10eca3ae8> ['planet2']\n",
      "<Element tr at 0x10eca3368> ['planet3']\n",
      "<Element tr at 0x10eca3818> ['planet4']\n",
      "<Element tr at 0x10ea48bd8> ['planet5']\n",
      "<Element tr at 0x10ec85728> ['planet6']\n",
      "<Element tr at 0x10ec85c78> ['planet7']\n",
      "<Element tr at 0x10ec850e8> ['planet8']\n",
      "<Element tr at 0x10eca3a98> ['planet9']\n"
     ]
    }
   ],
   "source": [
    "for v in tree.xpath(\"/html/body/div[1]/table/tr\"):\n",
    "    print (v, v.xpath(\"@id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each level of the XPath can return multiple elements in an array.  This array starts at 1 instead of 0 (a common source of errors).  So this statement states that we want the first div that is found.\n",
    "\n",
    "With this document we can also specify that we only want a div with an id attribute with a particular value, in this case \"planets\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element tr at 0x10ec8a138> ['planetHeader']\n",
      "<Element tr at 0x10ec8a1d8> ['planet1']\n",
      "<Element tr at 0x10ec8a3b8> ['planet2']\n",
      "<Element tr at 0x10ec8a228> ['planet3']\n",
      "<Element tr at 0x10ec8a2c8> ['planet4']\n",
      "<Element tr at 0x10ec8a098> ['planet5']\n",
      "<Element tr at 0x10ec8a0e8> ['planet6']\n",
      "<Element tr at 0x10ec8a458> ['planet7']\n",
      "<Element tr at 0x10ec8a4a8> ['planet8']\n",
      "<Element tr at 0x10eca3a98> ['planet9']\n"
     ]
    }
   ],
   "source": [
    "for v in tree.xpath(\"/html/body/div[@id='planets']/table/tr\"):\n",
    "    print (v, v.xpath(\"@id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to exclude the header row from this result.  There are several ways to do this.  The first can use the fact that the row has an id of \"planetHeader\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element tr at 0x10eca3f48> ['planet1']\n",
      "<Element tr at 0x10eca3f98> ['planet2']\n",
      "<Element tr at 0x10eca3548> ['planet3']\n",
      "<Element tr at 0x10eca39a8> ['planet4']\n",
      "<Element tr at 0x10eca3cc8> ['planet5']\n",
      "<Element tr at 0x10ec8a048> ['planet6']\n",
      "<Element tr at 0x10ec8a098> ['planet7']\n",
      "<Element tr at 0x10ec8a0e8> ['planet8']\n",
      "<Element tr at 0x10eca3a98> ['planet9']\n"
     ]
    }
   ],
   "source": [
    "for v in tree.xpath(\"/html/body/div[@id='planets']/table/tr[@id!='planetHeader']\"):\n",
    "    print (v, v.xpath(\"@id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use the fact that the planet rows have an attribute class with value of planet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element tr at 0x10eca3c78> ['planet1']\n",
      "<Element tr at 0x10eca39a8> ['planet2']\n",
      "<Element tr at 0x10eca3cc8> ['planet3']\n",
      "<Element tr at 0x10eca3598> ['planet4']\n",
      "<Element tr at 0x10eca3278> ['planet5']\n",
      "<Element tr at 0x10eca3548> ['planet6']\n",
      "<Element tr at 0x10eca3d18> ['planet7']\n",
      "<Element tr at 0x10eca3d68> ['planet8']\n",
      "<Element tr at 0x10eca3a98> ['planet9']\n"
     ]
    }
   ],
   "source": [
    "for v in tree.xpath(\"/html/body/div[@id='planets']/table/tr[@class='planet']\"):\n",
    "    print (v, v.xpath(\"@id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say the planet rows did not have attributes (nor the header row), then we could do this by position, skipping the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element tr at 0x10eca3908> ['Mercury']\n",
      "<Element tr at 0x10eca3638> ['Venus']\n",
      "<Element tr at 0x10eca3958> ['Earth']\n",
      "<Element tr at 0x10eca39a8> ['Mars']\n",
      "<Element tr at 0x10eca3228> ['Jupiter']\n",
      "<Element tr at 0x10eca31d8> ['Saturn']\n",
      "<Element tr at 0x10eca3278> ['Uranus']\n",
      "<Element tr at 0x10eca3548> ['Neptune']\n",
      "<Element tr at 0x10eca3a98> ['Pluto']\n"
     ]
    }
   ],
   "source": [
    "for v in tree.xpath(\"/html/body/div[1]/table/tr[position() > 1]\"):\n",
    "    print (v, v.xpath(\"@name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also navigate to the parent of a node using parent::*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element table at 0x10eca3598> ['planetsTable']\n",
      "<Element table at 0x10ebe7c28> ['footerTable']\n"
     ]
    }
   ],
   "source": [
    "for v in tree.xpath(\"/html/body/div/table/tr/parent::*\"):\n",
    "    print (v, v.xpath(\"@id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returned two got two parents as remember that this xpatch returns the rows from two tables, so the parents of all those rows are found.\n",
    "\n",
    "The * is a wild card that represents any parent tags with any name.  In this case, the two parents are both tables, but in general the result can be of any number of element types.  To specify a specific type, replace the * with the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element table at 0x10eca3908> ['planetsTable']\n",
      "<Element table at 0x10eca3ae8> ['footerTable']\n"
     ]
    }
   ],
   "source": [
    "for v in tree.xpath(\"/html/body/div/table/tr/parent::table\"):\n",
    "    print (v, v.xpath(\"@id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to specific a specific parent by position or attribute.  The following selects the parent with the id footerTable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element table at 0x10eca3ae8> ['footerTable']\n"
     ]
    }
   ],
   "source": [
    "for v in tree.xpath(\"/html/body/div/table/tr/parent::*[@id='footerTable']\"):\n",
    "    print (v, v.xpath(\"@id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A shortcut for parent is .. (and . also represents the current node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element table at 0x10ec849f8> ['planetsTable']\n",
      "<Element table at 0x10eca3ae8> ['footerTable']\n"
     ]
    }
   ],
   "source": [
    "for v in tree.xpath(\"/html/body/div/table/tr/..\"):\n",
    "    print (v, v.xpath(\"@id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following finds the mass of Earth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.97'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.xpath(\"/html/body/div[1]/table/tr[@name='Earth']/td[3]/text()[1]\")[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## How it works\n",
    "XPath is a element of the XSLT standard, and provides the ability to select nodes in an XML document.  Since HTML is a variant of HTML, XPath can also be used to find elements in HTML.\n",
    "\n",
    "lxml is a python library created for manipulating HTML document.  One of the features of lxml is the ability to execute XPath statements against a document, which lxml will then return the resulting set of nodes to your code.\n",
    "\n",
    "XPath itself is designed to model the structure of XML nodes, attributes and properties.  The systax provides means of finding items in the XML that match the expression.  This can include matching or logical comparison of any of the nodes, attributes, values or text in the XML document.  \n",
    "\n",
    "Expressions can be combined to form very complex paths within the document.  It is also possible to navigate the document based upon relative positions, which helps greatly in finding data based upon relative instead of absolute positions within the DOM.\n",
    "\n",
    "Understanding XPath is essential to knowing how to parse HTML and perform web scraping.  And as we will see, it underlies and provides an implementation for many of the higher level libraries such as lxml.  \n",
    "\n",
    "\n",
    "## There's more...\n",
    "Xpath is actually an amazing tool for working with XML and HTML documents.  It is quite rich in its capabilities, and we barely touched the surface of it's capabilities to demonstrate a few examples that are common to scraping data in HTML documents.  To learn much more, please visit the following links:\n",
    "\n",
    "* https://www.w3schools.com/xml/xml_xpath.asp\n",
    "* https://www.w3.org/TR/xpath/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSS Select\n",
    "CSS selectors are patterns used for selecting elements and are often used to define which elements that styles should be applied.  They can also be used with lxml to select nodes in the DOM. CSS selectors are common to usje as they are more compact than XPath and generally can be more reusable in code. As examples of common selectors which may be used:\n",
    "\n",
    "| What you are looking for | Example |\n",
    "| -- | -- |\n",
    "| All tags | * |\n",
    "| A sprecific tag (ie: tr) | tr |\n",
    "| A class name (ie: \"planet\") | .planet |\n",
    "| A tag with a class \"planet\" | tr.planet |\n",
    "| A tag with an ID \"planet3\" | tr#planet3 |\n",
    "| A child tr of a table | table > tr |\n",
    "| A descendant tr of a table | table tr |\n",
    "| A tag with an attribute (ie: tr with id=\"planet4\") | a[id=Mars] |\n",
    "\n",
    "## Getting ready\n",
    "Let's start examining css selectors by loading the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests\n",
    "page = requests.get(\"http://127.0.0.1:8080/pages/planets.min.html\")\n",
    "tree = html.fromstring(page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## How to do it..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following selects all tr elements with a class equal to \"planet\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element tr at 0x10ece1ae8> ['Mercury']\n",
      "<Element tr at 0x10ece1bd8> ['Venus']\n",
      "<Element tr at 0x10ece1e08> ['Earth']\n",
      "<Element tr at 0x10ed1aa48> ['Mars']\n",
      "<Element tr at 0x10ed1abd8> ['Jupiter']\n",
      "<Element tr at 0x10ed1ac28> ['Saturn']\n",
      "<Element tr at 0x10ed1ac78> ['Uranus']\n",
      "<Element tr at 0x10ed1acc8> ['Neptune']\n",
      "<Element tr at 0x10eca34a8> ['Pluto']\n"
     ]
    }
   ],
   "source": [
    "for v in tree.cssselect('tr.planet'):\n",
    "    print (v, v.xpath(\"@name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Data for the Earth can be found by several means.  The following gets the row based on ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Element tr at 0x10ece1e08>, 'Earth')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = tree.cssselect(\"tr#planet3\")\n",
    "tr[0], tr[0].xpath(\"./td[2]/text()\")[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or find with an attribute with a specific value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<Element tr at 0x10eca34a8>, 'Pluto')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = tree.cssselect(\"tr[name='Pluto']\")\n",
    "tr[0], tr[0].xpath(\"td[2]/text()\")[0].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that unlike XPath, the @ symbol need not be used to specify an attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works\n",
    "lxml converts the css selector you provide to XPath, and then performs that XPath expression against the underying document.  In essence, css selectors in lxml provide a shorthand to XPath that makes finding nodes fitting certain patterns simpler than with XPath.\n",
    "\n",
    "## There's more...\n",
    "Because css selectors utilize XPath under the covers, there is overhead to its used as compared to using XPath directly.  This difference is however almost a non-issue, and hence it certain scenarios it is easiest to just use cssselect.\n",
    "\n",
    "A full description of css selectors can be found at [Selectors Level 3](https://www.w3.org/TR/2011/REC-css3-selectors-20110929/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# Scrapy Selectors\n",
    "Scrapy is a Python web spider framework that is used to extract data from websites.  It's provides many powerful features for navigating entire websites, such as the ability to follow links.  A feature that it provides is the ability to find data within a document using the DOM, and using the now quite familiar XPath.\n",
    "\n",
    "The example we will look at will load the list of current questions on StackOverflow, and then parse this using a scrapy selector.  Using that selector, we will extract the text of each question.\n",
    "\n",
    "## Getting ready\n",
    "We start by importing Selector from scrapy, and also requests so that we can retrieve the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scrapy.selector import Selector\n",
    "import requests\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "We start with the loading of the page.  The following loads the 10 most recent questions from StackOverflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "payload = { 'pagesize': 1, 'sort': 'newest'}\n",
    "response = requests.get(\"http://stackoverflow.com/questions\", params=payload)\n",
    "response.url\n",
    "#response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a Selector by passing it the response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "selector = Selector(response)\n",
    "selector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use XPath to retreive the question summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "summaries = selector.xpath('//div[@class=\"summary\"]/h3')\n",
    "summaries[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the following prints all of the contained questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for question in summaries:\n",
    "    print (question.xpath('a[@class=\"question-hyperlink\"]/text()').extract()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works\n",
    "## There's more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "# How to load data in unicode / UTF-8\n",
    "A documents encoding tells an application how the characters in the document are represented as bytes in the file.  Essentially, the encoding specified how many bits there are per character.  In a standard ASCII document, all characters are 8-bits.  HTML files are often encoded as 8-bits per character, but with the globalization of the Internet, this is not always the case.  Many HTML documents are encoded as as 16-bit characters, or using a combination of 8 and 16-bit characters.\n",
    "\n",
    "A particularly common form of encoding of HTML documents is referred to as UTF-8.  This is the encoding form that we will examine.\n",
    "\n",
    "## Getting ready\n",
    "We will read a file named unicode.html.  This file is UTF-8 encoded and contains several sets of characters in different parts of the encoding space.  For example, the page looks like the following in your browser.\n",
    "\n",
    "![Unicode.html](img/01_04.png)\n",
    "\n",
    "Using an editor that supports UTF-8 we can see how the Cyrillic characters are rendered in the editor (in my case, Visual Studio Code).\n",
    "\n",
    "![Cyrillic](img/01_05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to do it...\n",
    "We will look at using urlopen and requests to handle HTML in UTF-8.  Let's start with urlopen.  The following reads the data and displays the section of the file for the Cyrillic table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "page = urlopen(\"http://localhost:8080/pages/unicode.html\")\n",
    "content = page.read()\n",
    "content[840:1280]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the Cyrillic characters were read in as multi-byte codes using \\ notation, such as \\xd0\\x89.  To rectify this, we can convert the content to UTF-8 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "str(content, \"utf-8\")[837:1270]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output now has the characters encoded properly.\n",
    "\n",
    "With requests we can do this in one statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get(\"http://localhost:8080/pages/unicode.html\")\n",
    "r.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it works\n",
    "In the case of using urlopen, the conversion was explicitly performed by using the str statement and specifying that the content should be converted to UTF-8.  For Requests, the library was able to determine from the content within the HTML that it was in UTF-8 format by seeing the following tag in the document:\n",
    "\n",
    "    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8\">\n",
    "\n",
    "\n",
    "## There's more...\n",
    "There are a number of resources available on the Internet to learn about Unicode and UTF-8 encoding techniques.  Perhaps the best is the following Wikipedia article with is an excellent summary and has a great table describing the encoding technique.\n",
    "\n",
    "https://en.wikipedia.org/wiki/UTF-8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
